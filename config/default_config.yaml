# # Default hyperparameters and parameters for pytorch training arguments that i will use when training the final model once the ideal.optimal parameters are found from the optuna optimization

default_params={
'num_epochs':2,
'per_device_train_batch_size':2,
'per_device_eval_batch_size':2,
'logging_strategy':'steps',
'logging_steps':100,
save_strategy:'steps',
'eval_strategy':'steps',
'save_steps':1000,
'eval_steps':1000,
'gradeint_accumulation_steps':8,
'graadient_checkpointing':True,
fp16:True,
'load_best_model_at_end':True,
'report_to':'wandb',
'eval_accumulation_steps':2,
'save_total_limit':2,
output_dir:'./results',
logging_dir:'./final_trainlogs',
}

MODEL_NAME='TinyLlama/TinyLlama-1.1B-Chat-v1.0'
Tokeninzer='TinyLlama/TinyLlama-1.1B-Chat-v1.0'